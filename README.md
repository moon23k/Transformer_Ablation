## Transformer_Ablation
Transformer는 다양한 인공지능 모델링에 사용되고 있습니다. 
Transformer의 기본적인 Architecture속에서 다양한 Design Choices마다의 성능 개선을 면밀하게 비교검증하면서,
제일 나은 Transformer구조를 만드는 것이 이 repo의 main purpose!

<br>

## Ablations

**Activation Function**
ReLU를 사용할지 GeLU를 사용할지 아니면 다른 Activation을 사용할지

<br>

**Recurrent Architecture**
일반화에 어떤 영향을 끼칠지

<br>

**Residual Connection**
Residual Connection를 먼저 적용할지 나중에 적용할지

<br>

**Positional Encoding**
Sinudial을 사용할지 아니면 Embedding Network를 사용해서 위치정보를 제공할지. 
위치정보만을 전달하는 것이라면, Sinudial을 사용하는게 파라미터를 줄일수 있기때문에 더 나을 것으로 가정함

<br>
<br>

## Result

<br>
