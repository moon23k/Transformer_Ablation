## Transformer_Ablation

Transformer is being used as a basis for various NLP models.
This repo focuses only on Transformer itself, comparing each Design Choices.


<br>
<br>

## Ablations

**Activation Function**

which activation func to use. ReLU, GeLU or something else

<br>

**Recurrent Architecture**



<br>

**Residual Connection**

Residual Connection를 먼저 적용할지 나중에 적용할지

<br>

**Positional Encoding**

Sinudial을 사용할지 아니면 Embedding Network를 사용해서 위치정보를 제공할지. 
위치정보만을 전달하는 것이라면, Sinudial을 사용하는게 파라미터를 줄일수 있기때문에 더 나을 것으로 가정함

<br>
<br>

## Configurations

<br>
<br>

## Result

<br>
